import os
import re
import pandas as pd
from transformers import BertTokenizer, BertForSequenceClassification
from torch.nn.functional import softmax
import torch

# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = BertTokenizer.from_pretrained("snunlp/KR-FinBERT")
model = BertForSequenceClassification.from_pretrained("snunlp/KR-FinBERT", num_labels=3)

# 2. ë¬¸ì¥ â†’ ê°ì„± ì ìˆ˜
def get_sentiment_probs(sentence):
    inputs = tokenizer(sentence, return_tensors="pt", truncation=True, max_length=512)
    outputs = model(**inputs)
    probs = softmax(outputs.logits, dim=1).detach().numpy()[0]
    return {
        'neg': float(probs[0]),
        'neu': float(probs[1]),
        'pos': float(probs[2])
    }

# 3. ë¬¸ì¥ ìœ ì‚¬ ë¶„ë¦¬ê¸° + ë§ˆì¹¨í‘œ ë³´ì •
def pseudo_sentence_split(text):
    # íŒ¨í„´ ê¸°ë°˜ êµ¬ë¬¸ ë‹¨ìœ„ ëŠê¸°
    text = re.sub(r'([ê°€-í£]{2,}(ìš”|ë‹¤|ì£ |ë„¤|ëŠ”ë°ìš”|ì§€ë§Œ|ì|êµ°ìš”|ê±°ë“ ìš”))(\s+)', r'\1.\3', text)

    # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ê°•ì œë¡œ ëŠê¸°
    sentences = []
    current = ""
    for token in text.strip().split():
        current += token + " "
        if len(current) > 120:
            sentences.append(current.strip() + ".")
            current = ""
    if current.strip():
        sentences.append(current.strip() + ".")
    return sentences

# 4. í…ìŠ¤íŠ¸ íŒŒì¼ â†’ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
def load_sentences(filepath):
    with open(filepath, 'r', encoding='utf-8') as file:
        text = file.read()
    return pseudo_sentence_split(text)

# 5. ë‚ ì§œë³„ ë¶„ì„ ìˆ˜í–‰
def analyze_transcripts(folder_path):
    results = []

    for file in sorted(os.listdir(folder_path)):
        if file.endswith('.txt'):
            date = file.replace('.txt', '')
            filepath = os.path.join(folder_path, file)

            sentences = load_sentences(filepath)
            pos_scores = []
            neg_scores = []

            for sentence in sentences:
                try:
                    probs = get_sentiment_probs(sentence)
                    pos_scores.append(probs['pos'])
                    neg_scores.append(probs['neg'])
                except Exception as e:
                    print(f"âŒ ë¬¸ì¥ ì²˜ë¦¬ ì‹¤íŒ¨: {sentence[:30]}... â†’ {e}")

            # í‰ê·  ê³„ì‚° ë° OPN ì ìˆ˜ ì¶”ì¶œ
            if pos_scores and neg_scores:
                avg_pos = sum(pos_scores) / len(pos_scores)
                avg_neg = sum(neg_scores) / len(neg_scores)
                opn_score = round(avg_pos - avg_neg, 4)
            else:
                avg_pos = avg_neg = opn_score = 0.0

            results.append({'date': date, 'opn_score': opn_score})

            # âœ… ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“… {date} ë¶„ì„ ê²°ê³¼:")
            print(f" - ì´ ë¬¸ì¥ ìˆ˜: {len(sentences)}")
            print(f" - ì²˜ë¦¬ëœ ë¬¸ì¥ ìˆ˜: {len(pos_scores)}")
            print(f" - í‰ê·  ê¸ì • ì ìˆ˜: {round(avg_pos, 4)}")
            print(f" - í‰ê·  ë¶€ì • ì ìˆ˜: {round(avg_neg, 4)}")
            print(f" âœ… OPN ì ìˆ˜: {opn_score}")

    return pd.DataFrame(results)

# 6. ì‹¤í–‰
if __name__ == "__main__":
    TRANSCRIPT_FOLDER = "../transcripts"
    OUTPUT_CSV = "../data/daily_opn.csv"

    df_opn = analyze_transcripts(TRANSCRIPT_FOLDER)
    df_opn.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')

    print("\nâœ… ëª¨ë“  ë‚ ì§œë³„ OPN ê°ì„± ì ìˆ˜ ì €ì¥ ì™„ë£Œ:")
    print(df_opn)